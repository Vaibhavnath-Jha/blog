[{"content":"Intro The inception of this journey happened when I wanted to utilize our decade old PC. It was a Dell Inspiron 660s. A small form factor PC which was quite capable back in 2013. After spending some time on Reddit and watching a couple of YouTube videos, I came across this interesting idea of Homelab. It basically means you host a server at home that locally runs different services you want to use. Here is a list of advantages from my perspective.\nYou have control over your data for maximum privacy.\nLast year, Google and Samsung had introduced a tool called Magic Eraser, which is a great tool when you want to remove unwanted objects from your picture. Turns out, if you cover your face with your hands and then erase your hands using this tool, then AI will fix the photo eerily close to the real deal. At this point you ask yourself, if the claim \u0026ldquo;We do not train our models on your photos\u0026rdquo; really true? Now that your data does not leave your server, you are in charge of everything. This is truly powerful and the most important benefit of this setup. No more running out of space issues.\nRemember that time you took a picture and got a warning that you are running low on space, 👀 iPhone users. That is going to be a thing of the past. Theoretically, you can have unlimited storage in this setup. Realistically, think of how many pictures/videos you can store with 1 TB of space. The feel good factor.\nOnce everything is setup and you cancel your, spoiler alert, Google Photos subscription, you get this happy feeling that you did something cool and get the bragging rights to talk about self-hosting. However, there are some points to consider before you begin with this project.\nIf you want your services up and running 24/7, then you have to leave your server on 24/7.\nEnergy consumption and cost of running the server is a real consideration here. Later, I will also talk about how to have an efficient system. You have to be comfortable with the shell environment. You could have a GUI but that requires more resources.\nBackup \u0026gt; Backup \u0026gt; Backup\nNow that you are in control of your data that also brings the responsibility of preserving that data in the long run. A disk failure can happen anytime without any warning. So make sure to backup your data regularly. This is still better than forgetting a recurring payment for Google One or iCloud and loosing all those photos forever. You have to invest your time and energy.\nWith the advent of LLMs it definitely gets easier to find the correct information, but at the end you have to find the time to execute these things, fix bugs, pull the latest updates and general maintenance of your infrastructure. IMHO, it is not a set-it-and-forget-it kind of a project. You have to be involved. This will be your baby now. But, I see this as a positive challenge. As you saw, the cons outweigh the pros 4:3. I leave the decision to the reader if they want to make the transition or not. With that out of the way, let\u0026rsquo;s talk tech now.\nComing up with a name I named this server, Rohini, after one of the stars (nakshatras) found in the ancient texts in Indian astronomy. Assigning a name to the server makes it easier for future references and helps in maintaining the infrastructure.\nChoosing the OS There were couple of options here. Ubuntu Server, OracleBox on another OS, ESXi or Proxmox. I chose Proxmox. It provides kernel level virtualization and is community driven but is also used in enterprise environment. The documentation is really rich and you can find what you are looking for fairly easily. I went into this with no prior experience with virtualization software but was able to manage by myself. That is the advantage of community driven development. You get a lot of support.\nHardware Here\u0026rsquo;s a list of what I am running Proxmox on.\nIntel core i3-2120 @3.6 GHz 4 GB DDR3 single memory 500 GB Crucial BX500 SSD, 500 GB decade old Seagate HDD m-ATX Dell proprietary Motherboard and 120W PSU. There\u0026rsquo;s a second RAM slot but for some reason I never got it to work. As soon as I plugged another RAM in that slot, I could not POST and got beep-beep-beep sounds from the MB.\nCD/DVD Writer Remember the good old days when we use to burn our favorite songs on CDs. I had to resort to this to install Proxmox on the system since the BIOS was soo old that I could not boot from a USB and the BIOS did not want to update and did not support UEFI for some stupid proprietary reason. Eventually, I got Proxmox up and running. The plan was simple, use the HDD for backups and SSD for live storage.\nPower Consumption These decade old PCs were not designed to run 24/7. Running something like that, I had to make sure I am just the necessary amount of power. There used to be a Nvidia GPU, I think it had 2GB of VRAM and it was model no. 560(?). But I removed it, since I did not require it for my needs and all it would do was draw more power. I turned off the on-board WiFi card since I am plugged in via the ethernet. Finally, there were some Proxmox tweaks that I enabled to reduce the power consumption when idle. Here\u0026rsquo;s the bash script,\n#!/bin/bash # LXC Container Control Script, make_idle.sh # Usage: ./containers.sh [start|stop] CONTAINERS=(\u0026#34;102\u0026#34; \u0026#34;103\u0026#34; \u0026#34;104\u0026#34;) # Add container IDs if [[ $# -ne 1 ]]; then echo \u0026#34;Error: Please specify \u0026#39;start\u0026#39; or \u0026#39;stop\u0026#39;\u0026#34; exit 1 fi case $1 in start) for container in \u0026#34;${CONTAINERS[@]}\u0026#34;; do echo \u0026#34;Starting $container...\u0026#34; pct start $container done echo \u0026#34;Enabling performance mode...\u0026#34; echo \u0026#34;performance\u0026#34; | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor ;; stop) for container in \u0026#34;${CONTAINERS[@]}\u0026#34;; do echo \u0026#34;Stopping $container...\u0026#34; pct stop $container done echo \u0026#34;Enabling powersave mode...\u0026#34; echo \u0026#34;powersave\u0026#34; | tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor ;; *) echo \u0026#34;Invalid argument: Use \u0026#39;start\u0026#39; or \u0026#39;stop\u0026#39;\u0026#34; exit 1 ;; esac echo \u0026#34;Operation completed for containers: ${CONTAINERS[*]}\u0026#34; When I do not want to use the server, I just run ./make_idle.sh stop from my Proxmox mobile client noVNC console. It stops the resource hungry containers, [immich, nextcloud, dashboard] and sets the scaling governor to powersave mode. This way the power draw at idle is around 65W, which is still a lot but that\u0026rsquo;s what you get for running old PCs. The good thing about this is when I want to use these services I just run the script again with the start keyword and all the containers just resume from their last state.\nRunning services All the services that I have running at the moment are running in their individual LXC containers. I got most of the services up and running using the Proxmox Helper Scripts and the first service that I installed was,\nNote: Keep in mind that Proxmox-Helper-Scripts has seen a change of administration and the outlook, at the moment, is that it is not what it used to be.\nTailscale It is a wireguard based VPN technology that relies on their relay servers to form a mesh network for your devices. This was of utmost importance since this server will be hosted at my parents place in India and I live in Germany. I should be able to connect to my server even when I am 6000 kms away. And in this regard Tailscale works like magic. Best part is you do not have to forward any port on your router. This is nice because I am still a little skeptical about opening a port on my router towards the public internet since I lack proper knowledge and experience to make things secure when it comes to networking. Tailscale enables end-to-end encrypted, point-to-point connections between devices without routing traffic through a central VPN server, which reduces latency and enhances security. The relay servers are just used to authenticate the devices in the tailnet by exchanging public-private keypair.\nImmich Immich is a self-hosted drop in replacement of Google Photos. Essentially, Immich is running inside a docker container which is running inside a LXC container. I know this situation is like the Russian doll and I would like to fix this in the future. The transition itself was pretty smooth. Getting the photos out of Google Photos was time-consuming but easy. There is this service called Google Takeout which you can use to download all the data that you have given to Google. You won\u0026rsquo;t believe how much of that can amount to. I was already on the Google One 100 GB plan and after requesting the data it took Google some hours to generate the download links. Once you have the data you can use Immich-go another open-source software to manage your transfer from Google Photos to Immich. The setup was self explanatory and worked like a charm. It took me 4.5 hrs on 250 Mbps UP/DOWN speed to upload the data to the SSD.\nNote: I could have done this much better by copying the data to an external drive and then offload the same on the SSD on the server. Finally, I could have told Immich to just use this source as an external library. But I learnt this later. Soo moving on.\nToday, Immich is working really great. Now, my photos directly get backed up to the server from my mobile client. I take periodic backups to ensure I do not loose anything in case of a failure. The experience was soo great that I got my father and my wife on this service as well.\nNextcloud Now, Nextcloud, similar to immich is a self hosted replacement for Google Drive. And again, I used the Proxmox Helper Script to install the service. I don\u0026rsquo;t use this service as much I would like it too since I have not imported all of my data from Google Drive to Nextcloud, yet. Something for the future.\nHomepage Homepage is a simple dashboard for your services so that you can find all your services in one place. It is highly configurable and can also talk to other services using API calls.\nPihole Pihole is my ad-blocker and DNS resolver of choice. This is crucial for your infrastructure and who does not like network wide ad-blocking. The real benefit is when you configure Pihole to be the Global DNS resolver in your tailnet so whenever you are connected to your tailnet you can still block those pesky ads wherever you are.\nCaddy Caddy is a great reverse-proxy written in Go that automatically obtains and renews the TLS certificates for your sites. It is recommended for its ease of use and the community offers a variety of plugins. This became a crucial service when I wanted to use my purchased domain for reverse-proxying the services and obtain TLS certificate with DNS-ACME challenge via Hetzner API. This can be a blogpost of its own. Long story short, after setting this up, I could reach, for e.g., Nextcloud using cloud.vaibhavnath.in.\nOutro This was just an introduction of an even bigger project. I will continue this series where I extend my Homelab with 2 more nodes and one NAS. I will also write about automatic backups of entire LXC using BorgBackup. Getting a Hetzner Storage Box for achieving true 3-2-1 backup policy. So stay tuned.\nUntil then, 😊 VJ\n","permalink":"https://blog.vaibhavnath.in/posts/2025/09/degoogling-my-life/","summary":"This is my journey of getting independence from the data overlords at Google, one service at a time.","title":"Degoogling my life - Part I"},{"content":"We spontaneously decided two weeks ago, and yes that is spontaneous enough by the european standards, to visit Salzburg and Hallstatt on Friday, 8th August and return back on 11th August. Through our friend circle, we already knew that there\u0026rsquo;s the largest Salt mines of Europe in the Hallstatt region that is open for visitors. This was one of the things that we were looking forward to and apart from that we did not really have a concrete itinerary. The plan was to just take some time off from the usual routine and relax a bit in the chill town of Salzburg.\nWe started by taking the earliest train at 0557 hrs to Munich from Braunschweig (BS) on the 8th. We reached Munich by 1045 hrs. Thanks to the speed of ICE trains we covered around 450 kilometers in 4.75 hrs. In some stretches the speed even reached 350 kmph, Oof! After reaching Munich we had to continue to Salzburg. Unfortunately, there were no high speed trains that week due to construction work. Therefore, we travelled another 3 hours via Regional Bahn and reached Salzburg Hbf by 1400 hrs. After getting a quick lunch we made our way to the hotel, COOL MAMA.\nLater the same day, we explored the city a bit and went to the Augustiner Bräu Mülln. To end the day, we got ourselves a mug of Märzbier which is traditional bew from the famous Oktober fest.\nOn the next day, we made an itinerary to explore the city a little better. We started from our hotel, visited Mozart\u0026rsquo;s Birthplace, his residence and then the Mozartplatz. The next stop was Festung Hohensalzburg. A fortress built on top of a hill in the middle of the city from the medieval times. We took the funicular to the base of the fortress and after having a pint (since it was freaking 37 degrees that day) we explored the fortress. The views from the top did however made up for the heat. Kind of reminded us of Heidelberg.\nAfter a good night of sleep, we travelled to Hallstatt. The morning itself got a little thrilling than we had hoped for. The bus from the hotel ran 10 minutes late and we reached the Hbf 20 minutes later than scheduled as a result we missed the connection we had planned for. Now, since we had a flexible ticket it did not really matter much, we just took the next bus to Bad-Ischl after that we had to continue to Hallstatt by train and then finally cross the river by a ferry. While booking these ticket I fortunately found out that we can directly book the tickets to the Salt mines on OBB website, as a combo offer (48€ pp) . This was great, since the salt mine company don\u0026rsquo;t offer online tickets. The moment we got off the train, the views were breathtaking. I will now stop writing and let the pictures justify my point.\nUntil then, 😊 VJ\n","permalink":"https://blog.vaibhavnath.in/posts/2025/08/trip-to-salzburg-hallstatt/","summary":"A four day trip to the Musical town Salzburg and the picturesque Hallstatt, Austria.","title":"Trip to Salzburg and Hallstatt"},{"content":"Following the work from my first post. I wanted to now re-direct the APEX domain, that is, vaibhavnath.in to blog.vaibhavnath.in.\nSwitching Name servers At this stage each request made to the domain gets served by GoDaddy\u0026rsquo;s servers and this is fine until we have to do something a little more than just basic DNS forwarding. Here comes Hetzner. It is a german company which provides dedicated servers and several hosting options. Its DNS management frontend is simple, modern and offers a powerful API for further automation. Although, that was not the only reason why I chose Hetzner. DNS management came as an add-on I was already using Hetzner for something else but that\u0026rsquo;s a blog series in itself. 😉\nFirst we start with telling GoDaddy that different name servers will be used. For this, we again head to GoDaddy\u0026rsquo;s Domain page. Then navigate to DNS \u0026gt; Nameservers. Here you can add the custom nameserver address. Since, I am going to use Hetzner, the addresses are hydrogen.ns.hetzner.com and oxygen.ns.hetzner.com. You should find the addresss for your respective DNS management service. Once done, we can log out of GoDaddy and head to Hetzner to configure further.\nDNS Records in Hetzner We first start with defining a zone. This is nothing but the domain-name that you have registered. All records related to this domain will be further created in this zone. Once done, we create a CNAME record where Name=blog.vaibhavnath.in and Value=vaibhavnath-jha.github.io.. Do mind the dot at the end of the Value string. This tells Hetzner that the Value is the apex/root domain and to not concatenate the zone-name at the end.\nAfter completing the above step we have successfully replaced GoDaddy. Now, we can visit this page via blog.vaibhavnath.in. Now, for the main part, we make our way to Netlify.\nNetlify business Netlify is another cloud computing company where we can deploy a serverless backend service for free. Here, we manually deploy a small website whose single job is to re-direct to blog.vaibhavnath.in. For this purpose, create a file wihtout any extension and name it _redirects. In this file, I added the following,\n/* https://blog.vaibhavnath.in/:splat 301! We need to create another file index.html but this can be left blank.\nNext, we zip these two files, drag and drop this zipped file in manual upload section in Netlify, and deploy our website. Once deployed, Netlify will generate an address for this website. In my case, it was zesty-banoffee-0d89d4.netlify.app. And if you visit this site, you will be re-directed to blog.vaibhavnath.in (it is getting repetitive, isn\u0026rsquo;t it 🤐).\nFinally, I can add my registered domain as a custom domain in the Domain Management section in Netlify. Here\u0026rsquo;s a screenshot. .\nAnother CNAME record in Hetzner Our job is now 80% done. All we have to do is tell Hetzner to link my apex domain with the Netlify address. This is a repeated step, we create another CNAME record this time the values are Name=www.vaibhavnath.in and Value=zesty-banoffee-0d89d4.netlify.app..\nAdding an A record The last step is to add another record, this time it is an A record, short for Address record. This effectively activates the CNAME record we created earlier by pointing the host to an IPv4 address. The address in question here would be the Netlify\u0026rsquo;s IPv4 address. This is how the records table is looking in my DNS management panel. Once all this is configured and the domain name is propogated completely with Netlify\u0026rsquo;s server. It should automatically fetch an TLS certifcate and you will have a secure address, i.e., https://www.vaibhavnath.in\nOutro This completes the blog101 series. Next, I start a series on which I have been working on for the last 9 months. So, you can call it my baby. A small hint, it has to do with ☁️.\nUpdate 26.09.2025: The blog about ☁️ is out, you can read it here.\nUntil then, 😊 VJ\n","permalink":"https://blog.vaibhavnath.in/posts/2025/05/apex-domain-and-dns/","summary":"Using Hetzner\u0026rsquo;s name server instead of GoDaddy and re-directing the apex domain to this site.","title":"Apex Domain and DNS"},{"content":"In this very first blog post, I will be covering how to deploy your static site generated by hugo on Gh-Pages at almost no cost.\nInstalling Go, Hugo and Git The preliminary steps is to install all the necessary software required to build our static website.\nHugo\nGit We require Git for installing the preffered theme as a Git submodule and to have version control for our work. Eventually we\u0026rsquo;ll use Git and GitHub to deploy our website.\nThe easiest way to install both the software packages is through winget, the package manager for Windows system. Similar package managers are avaialable for both Linux (apt, yum, pacman) and MacOS (homebrew).\nRun winget install Hugo.Hugo.Extended to install Hugo and run winget install --id Git.Git -e --source winget to install Git.\nYou can also choose to build Hugo from source and for that you require Go. While it is not necessary to install Go, it is recommended nevertheless.\nOnce Hugo is installed you can run hugo version to verify the installation. If the command returns with something like this, hugo v0.137.0-...+extended windows/amd64 BuildDate=2024-11-04T16:04:06Z VendorInfo=gohugoio, it implies that the installation was successful.\nChoosing the theme, paperMod We can now proceed to select our desired theme for the website. Many themes are avaialable at the Hugo themes section and you can choose one according to your needs. For me, the choice is paperMod. The theme\u0026rsquo;s documentation is well laid out and covers all the basics you need to know. You also get a working demo of the theme and get a good idea of how your own site would look like.\nMy preffered way to get the theme was adding it as a Git submodule. This required a couple of commands as explained in the theme\u0026rsquo;s documentation, viz.,\ngit submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod git submodule update --init --recursive This method enables one liner update process, I can just run git submodule update --remote --merge in the root of the project to update the theme.\nPopulating some content Now that we have all that is required to build the site we can start cooking. All the configuration for the site are listed in the hugo.yaml file. Each option is again explained in the documentation.\nAdding some content is then easy and straight forward. All you have to do is run hugo new \u0026lt;your-folder-structure/filename.md\u0026gt;. This will create a markdown file in the folder structure you want to place the file into. A quick tip, create the folder with the name you want and place the file index.md in there. This creates a page bundle that means we can place assets in the same directory. You can add images to your page using markdown format, like ![Descriptive Alt Text](image.jpg \u0026quot;Optional Title\u0026quot;). Once you have some content down in the file you can render your site by running hugo server and visit the site on localhost:1313.\nExtending the head.html You can tinker a lot with the hugo.yaml to get the configuration you want for your site and everything was working fine. Until, I pushed all the changes to Github and published the site on GitHub-Pages. All the stylesheets were broken and the assets were gone. For some reason the url was not being resolved properly for these properties. After a little digging online, I found out that I have to specify absURL in the stylesheet and all the favicon references in the .\\themes\\PaperMod\\layouts\\partials\\head.html. We can do this by creating our own head.html at .\\layouts\\partial\\. This will sort of extend the themes\u0026rsquo; template and use the address provided in this file to link the stylesheets and the asset correctly.\nThe static directory Another challenge that I have for now is that all the assets that I place in the .\\static folder gets dumped into the root of the directory after building the site. Perhaps there is a fix. For now, this will do just fine.\nDeploying on GitHub pages With all these changes. I was again ready to deploy on Github-Pages and since I had done this in the past with another site. This step was a bit familiar to say the least.\nCustom domain ($) The only place where I had to invest was for securing a domain for myself and this step is entirely optional. For setting up the custom domain, firstly, you need to buy a domain of your choice and these days getting a domain is really resonable. You can choose any domain registrar for this purpose. I went with GoDaddy and bought the domain for 1500/- INR for 3 years. After three years, it will be 520/- INR per year. The price depends on the top level domain (TLD). For e.g., vaibhavnath.io was costlier than vaibhavnath.in.\nOnce you have access to your domain, you need to update the baseURl in your hugo.yaml config file and set it to your preffered subdomain. In my case it was baseURL: https://blog.vaibhavnath.in/. Do not forget the trailing slash, it is recommended to add that in the URL.\nNext, go to your website\u0026rsquo;s repository on GitHub, navigate to Settings \u0026gt; Pages and under Custom domain, fill in the box with the custom domain you entered in the baseURL. Here, without the protocol and the trailing slash, i.e., blog.vaibhavnath.in\nMulitple GitHub-pages config GitHub allows having many repository with GitHub pages. If you have more than one Github-pages, you need to use the base URL of the first page you had created and then use the sub-directory of second repository to acces its GitHub page.\nFor example, in my case the primary Repo had a Github page at https://vaibhavnath-jha.github.io and the name of your second repository is blog. Therefore, the Github page for the second repo would be reachable at https://vaibhavnath-jha.github.io/blog/. However, Github will redirect the request to the custom domain.\nDomain Forwarding at GoDaddy Once everything is configured we can setup Domain Forwarding from GoDaddy\u0026rsquo;s dashboard. This will allow our request to be served from the subdomain to the intended GitHub-page\u0026rsquo;s web address. If we do not do this, we cannot visit the site via blog.vaibhavnath.in\nSetting this up is straightforward. From GoDaddy\u0026rsquo;s dashboard, you have to navigate to Domain \u0026gt; DNS \u0026gt; Forwarding and then fill up the form as you have configured your site. Below, there\u0026rsquo;s a screenshot of the form appropriate for my use-case. Enforcing HTTPS After a crucial wait of 24 hours. You can enforce HTTPS on your static site. Github will run some DNS check to see if the DNS can be resolved successfully or not. Once done, your website will then have a valid certificate.\nAPEX domain and Name servers I continue this topic in Part 2. Stay tuned.\nUntil then, 😄 VJ\n","permalink":"https://blog.vaibhavnath.in/posts/2024/11/setting-up-the-blog-page/","summary":"Setting up the blog page and tackling few challenges on the way to make it work.","title":"Hello World"}]